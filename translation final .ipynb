{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, GRU, LSTM, Bidirectional,RepeatVector,TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.initializers import Constant\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('deu.txt', sep=\"\\t\", header=None)\n",
    "data.columns = [\"english\", \"german\", \"garbage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>german</th>\n",
       "      <th>garbage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Geh.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hallo!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Grüß Gott!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Lauf!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run.</td>\n",
       "      <td>Lauf!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  english      german                                            garbage\n",
       "0     Go.        Geh.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "1     Hi.      Hallo!  CC-BY 2.0 (France) Attribution: tatoeba.org #5...\n",
       "2     Hi.  Grüß Gott!  CC-BY 2.0 (France) Attribution: tatoeba.org #5...\n",
       "3    Run!       Lauf!  CC-BY 2.0 (France) Attribution: tatoeba.org #9...\n",
       "4    Run.       Lauf!  CC-BY 2.0 (France) Attribution: tatoeba.org #4..."
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('garbage', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>german</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Geh.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hallo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Grüß Gott!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run.</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  english      german\n",
       "0     Go.        Geh.\n",
       "1     Hi.      Hallo!\n",
       "2     Hi.  Grüß Gott!\n",
       "3    Run!       Lauf!\n",
       "4    Run.       Lauf!"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    text = pattern.sub('', text)\n",
    "    text = \" \".join(filter(lambda x:x[0]!='@', text.split()))\n",
    "    emoji = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001FFFF\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    text = emoji.sub(r'', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)        \n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text) \n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)  \n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)  \n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"don't\", \"do not\", text)\n",
    "    text = re.sub(r\"did't\", \"did not\", text)\n",
    "    text = re.sub(r\"can't\", \"can not\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"couldn't\", \"could not\", text)\n",
    "    text = re.sub(r\"have't\", \"have not\", text)\n",
    "    text = re.sub(r\"[,.\\\"\\'!@#$%^&*(){}?/;`~:<>+=-]\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanTokenize(df):\n",
    "    sentences = list()\n",
    "    lines = df[\"english\"].values.tolist()\n",
    "\n",
    "    for line in lines:\n",
    "        line = clean_text(line)\n",
    "        # tokenize the text\n",
    "        tokens = word_tokenize(line)\n",
    "        # remove puntuations\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        # remove non alphabetic characters\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        # remove stop words\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        sentences.append(words)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['go'],\n",
       " ['hi'],\n",
       " ['hi'],\n",
       " ['run'],\n",
       " ['run'],\n",
       " ['wow'],\n",
       " ['wow'],\n",
       " ['fire'],\n",
       " ['help'],\n",
       " ['help']]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_sentences = CleanTokenize(data)\n",
    "eng_sentences[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(eng_sentences, open('english.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_german(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    text = pattern.sub('', text)\n",
    "    text = \" \".join(filter(lambda x:x[0]!='@', text.split()))\n",
    "    emoji = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001FFFF\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    text = emoji.sub(r'', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[,.\\\"\\'!@#$%^&*(){}?/;`~:<>+=-]\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanTokenize_german(df):\n",
    "    sentences = list()\n",
    "    lines = df[\"german\"].values.tolist()\n",
    "\n",
    "    for line in lines:\n",
    "        line = clean_text_german(line)\n",
    "        # tokenize the text\n",
    "        tokens = word_tokenize(line)\n",
    "        # remove puntuations\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        # remove non alphabetic characters\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        sentences.append(words)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['geh'],\n",
       " ['hallo'],\n",
       " ['grüß', 'gott'],\n",
       " ['lauf'],\n",
       " ['lauf'],\n",
       " ['potzdonner'],\n",
       " ['donnerwetter'],\n",
       " ['feuer'],\n",
       " ['hilfe'],\n",
       " ['zu', 'hülf']]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ger_sentences = CleanTokenize_german(data)\n",
    "ger_sentences[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(eng_sentences, open('german.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(line) for line in eng_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(line) for line in ger_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique tokens -  34239\n",
      "vocab size - 15862\n"
     ]
    }
   ],
   "source": [
    "max_length = 44\n",
    "\n",
    "tokenizer_eng = Tokenizer()\n",
    "tokenizer_eng.fit_on_texts(eng_sentences)\n",
    "eng_sequences = tokenizer_eng.texts_to_sequences(eng_sentences)\n",
    "\n",
    "word_index_eng = tokenizer_eng.word_index\n",
    "print(\"unique tokens - \",len(word_index_eng))\n",
    "vocab_size = len(tokenizer_eng.word_index) + 1\n",
    "print('vocab size -', vocab_size)\n",
    "\n",
    "eng_pad = pad_sequences(eng_sequences, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208486, 44)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([680,  47,   3,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_pad[170253]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique tokens -  34239\n",
      "vocab size - 34240\n"
     ]
    }
   ],
   "source": [
    "max_length = 76\n",
    "\n",
    "tokenizer_ger = Tokenizer()\n",
    "tokenizer_ger.fit_on_texts(ger_sentences)\n",
    "ger_sequences = tokenizer_ger.texts_to_sequences(ger_sentences)\n",
    "\n",
    "word_index_ger = tokenizer_ger.word_index\n",
    "print(\"unique tokens - \",len(word_index))\n",
    "vocab_size = len(tokenizer_ger.word_index) + 1\n",
    "print('vocab size -', vocab_size)\n",
    "\n",
    "ger_pad = pad_sequences(ger_sequences, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208486, 76)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ger_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,   18,   22, 1591, 1214,  871,    8,   61,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ger_pad[170253]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(41697, 76, 1)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.arange(eng_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "eng_pad = eng_pad[indices]\n",
    "ger_pad = ger_pad[indices]\n",
    "\n",
    "num_validation_samples = int(0.2 * ger_pad.shape[0])\n",
    "\n",
    "X_train_pad = eng_pad[:-num_validation_samples]\n",
    "y_train = ger_pad[:-num_validation_samples]\n",
    "y_train = np.asarray(y_train, dtype=np.float32)\n",
    "print(type(y_train))\n",
    "y_train = np.expand_dims(y_train, axis=2)\n",
    "X_test_pad = eng_pad[-num_validation_samples:]\n",
    "y_test = ger_pad[-num_validation_samples:]\n",
    "y_test = np.asarray(y_test, dtype=np.float32)\n",
    "print(type(y_test))\n",
    "y_test = np.expand_dims(y_test, axis=2)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166789, 44)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166789, 76, 1)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41697, 44)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41697, 76, 1)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "embedding_dim = 300\n",
    "GLOVE_DIR = \"O:\\Glove\"\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.300d.txt'), encoding = \"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15309\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word_index_eng) + 1, embedding_dim))\n",
    "c = 0\n",
    "for word, i in word_index_eng.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        c+=1\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15862, 300)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.3712e-01 -2.1691e-01 -6.6365e-03 -4.1625e-01 -1.2555e+00 -2.8466e-02\n",
      " -7.2195e-01 -5.2887e-01  7.2085e-03  3.1997e-01  2.9425e-02 -1.3236e-02\n",
      "  4.3511e-01  2.5716e-01  3.8995e-01 -1.1968e-01  1.5035e-01  4.4762e-01\n",
      "  2.8407e-01  4.9339e-01  6.2826e-01  2.2888e-01 -4.0385e-01  2.7364e-02\n",
      "  7.3679e-03  1.3995e-01  2.3346e-01  6.8122e-02  4.8422e-01 -1.9578e-02\n",
      " -5.4751e-01 -5.4983e-01 -3.4091e-02  8.0017e-03 -4.3065e-01 -1.8969e-02\n",
      " -8.5670e-02 -8.1123e-01 -2.1080e-01  3.7784e-01 -3.5046e-01  1.3684e-01\n",
      " -5.5661e-01  1.6835e-01 -2.2952e-01 -1.6184e-01  6.7345e-01 -4.6597e-01\n",
      " -3.1834e-02 -2.6037e-01 -1.7797e-01  1.9436e-02  1.0727e-01  6.6534e-01\n",
      " -3.4836e-01  4.7833e-02  1.6440e-01  1.4088e-01  1.9204e-01 -3.5009e-01\n",
      "  2.6236e-01  1.7626e-01 -3.1367e-01  1.1709e-01  2.0378e-01  6.1775e-01\n",
      "  4.9075e-01 -7.5210e-02 -1.1815e-01  1.8685e-01  4.0679e-01  2.8319e-01\n",
      " -1.6290e-01  3.8388e-02  4.3794e-01  8.8224e-02  5.9046e-01 -5.3515e-02\n",
      "  3.8819e-02  1.8202e-01 -2.7599e-01  3.9474e-01 -2.0499e-01  1.7411e-01\n",
      "  1.0315e-01  2.5117e-01 -3.6542e-01  3.6528e-01  2.2448e-01 -9.7551e-01\n",
      "  9.4505e-02 -1.7859e-01 -3.0688e-01 -5.8633e-01 -1.8526e-01  3.9565e-02\n",
      " -4.2309e-01 -1.5715e-01  2.0401e-01  1.6906e-01  3.4465e-01 -4.2262e-01\n",
      "  1.9553e-01  5.9454e-01 -3.0531e-01 -1.0633e-01 -1.9055e-01 -5.8544e-01\n",
      "  2.1357e-01  3.8414e-01  9.1499e-02  3.8353e-01  2.9075e-01  2.4519e-02\n",
      "  2.8440e-01  6.3715e-02 -1.5483e-01  4.0031e-01  3.1543e-01 -3.7128e-02\n",
      "  6.3363e-02 -2.7090e-01  2.5160e-01  4.7105e-01  4.9556e-01 -3.6401e-01\n",
      "  1.0370e-01  4.6076e-02  1.6565e-01 -2.9024e-01 -6.6949e-02 -3.0881e-01\n",
      "  4.8263e-01  3.0972e-01 -1.1145e-01 -1.0329e-01  2.8585e-02 -1.3579e-01\n",
      "  5.2924e-01 -1.4077e-01  9.1763e-02  1.3127e-01 -2.0944e-01  2.2327e-02\n",
      " -7.7692e-02  7.7934e-02 -3.3067e-02  1.1680e-01  3.2029e-01  3.7749e-01\n",
      " -7.5679e-01 -1.5944e-01  1.4964e-01  4.2253e-01  2.8136e-03  2.1328e-01\n",
      "  8.6776e-02 -5.2704e-02 -4.0859e-01 -1.1774e-01  9.0621e-02 -2.3794e-01\n",
      " -1.8326e-01  1.3115e-01 -5.5949e-01  9.2071e-02 -3.9504e-02  1.3334e-01\n",
      "  4.9632e-01  2.8733e-01 -1.8544e-01  2.4618e-02 -4.2826e-01  7.4148e-02\n",
      "  7.6584e-04  2.3950e-01  2.2615e-01  5.5166e-02 -7.5096e-02 -2.2308e-01\n",
      "  2.3775e-01 -4.5455e-01  2.6564e-01 -1.5137e-01 -2.4146e-01 -2.4736e-01\n",
      "  5.5214e-01  2.6819e-01  4.8831e-01 -1.3423e-01 -1.5918e-01  3.7606e-01\n",
      " -1.9834e-01  1.6699e-01 -1.5368e-01  2.4561e-01 -9.2506e-02 -3.0257e-01\n",
      " -2.9493e-01 -7.4917e-01  1.0567e+00  3.7971e-01  6.9314e-01 -3.1672e-02\n",
      "  2.1588e-01 -4.0739e-01 -1.5264e-01  3.2296e-01 -1.2999e-01 -5.0129e-01\n",
      " -4.4231e-01  1.6904e-02 -1.1459e-02  7.2293e-03  1.1026e-01  2.1568e-01\n",
      " -3.2373e-01 -3.7292e-01 -9.2456e-03 -2.6769e-01  3.9066e-01  3.5742e-01\n",
      " -6.0632e-02  6.7966e-02  3.3830e-01  6.5747e-02  1.5794e-01  4.7155e-02\n",
      "  2.3682e-01 -9.1370e-02  6.4649e-01 -2.5491e-01 -6.7940e-01 -6.9752e-01\n",
      " -1.0145e-01 -3.6255e-01  3.6967e-01 -4.1295e-01  8.2724e-02 -3.5053e-01\n",
      " -1.7564e-01  8.5095e-02 -5.7724e-01  5.0252e-01  5.2180e-01  5.7327e-02\n",
      " -7.9754e-01 -3.7770e-01  7.8149e-01  2.4597e-01  6.0672e-01 -2.0082e-01\n",
      " -3.8792e-01  4.1295e-01 -1.6143e-01  1.0427e-02  4.3197e-01  4.6297e-03\n",
      "  2.1185e-01 -2.6606e-01 -5.8740e-02 -5.1003e-01  2.8524e-01  1.3627e-02\n",
      " -2.7346e-01  6.1848e-02 -5.7901e-01 -5.1136e-01  3.6382e-01  3.5144e-01\n",
      " -1.6501e-01 -4.6041e-01 -6.4742e-02 -6.8310e-01 -4.7427e-02  1.5861e-01\n",
      " -4.7288e-01  3.3968e-01  1.2092e-03  1.6018e-01 -5.8024e-01  1.4556e-01\n",
      " -9.1317e-01 -3.7592e-01 -3.2950e-01  5.3465e-01  1.8224e-01 -5.2265e-01\n",
      " -2.6209e-01 -4.2458e-01 -1.8034e-01  9.9502e-02 -1.5114e-01 -6.6731e-01\n",
      "  2.4483e-01 -5.6630e-01  3.3843e-01  4.0558e-01  1.8073e-01  6.4250e-01]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_index.get(\"hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.37119997e-01 -2.16910005e-01 -6.63649989e-03 -4.16249990e-01\n",
      " -1.25549996e+00 -2.84659993e-02 -7.21949995e-01 -5.28869987e-01\n",
      "  7.20850006e-03  3.19970012e-01  2.94250008e-02 -1.32360002e-02\n",
      "  4.35110003e-01  2.57160008e-01  3.89950007e-01 -1.19680002e-01\n",
      "  1.50350004e-01  4.47620004e-01  2.84069985e-01  4.93389994e-01\n",
      "  6.28260016e-01  2.28880003e-01 -4.03849989e-01  2.73640007e-02\n",
      "  7.36790011e-03  1.39950007e-01  2.33459994e-01  6.81219995e-02\n",
      "  4.84219998e-01 -1.95780005e-02 -5.47510028e-01 -5.49830019e-01\n",
      " -3.40909995e-02  8.00170004e-03 -4.30649996e-01 -1.89689994e-02\n",
      " -8.56700018e-02 -8.11230004e-01 -2.10800007e-01  3.77840012e-01\n",
      " -3.50459993e-01  1.36840001e-01 -5.56609988e-01  1.68349996e-01\n",
      " -2.29519993e-01 -1.61840007e-01  6.73449993e-01 -4.65970010e-01\n",
      " -3.18339989e-02 -2.60369986e-01 -1.77970007e-01  1.94359999e-02\n",
      "  1.07270002e-01  6.65340006e-01 -3.48360002e-01  4.78329994e-02\n",
      "  1.64399996e-01  1.40880004e-01  1.92039996e-01 -3.50089997e-01\n",
      "  2.62360007e-01  1.76259995e-01 -3.13670009e-01  1.17090002e-01\n",
      "  2.03779995e-01  6.17749989e-01  4.90750015e-01 -7.52099976e-02\n",
      " -1.18150003e-01  1.86849996e-01  4.06789988e-01  2.83190012e-01\n",
      " -1.62900001e-01  3.83879989e-02  4.37940001e-01  8.82240012e-02\n",
      "  5.90460002e-01 -5.35149984e-02  3.88190001e-02  1.82019994e-01\n",
      " -2.75990009e-01  3.94739985e-01 -2.04990000e-01  1.74109995e-01\n",
      "  1.03150003e-01  2.51170009e-01 -3.65420014e-01  3.65280002e-01\n",
      "  2.24480003e-01 -9.75510001e-01  9.45049971e-02 -1.78590000e-01\n",
      " -3.06879997e-01 -5.86329997e-01 -1.85259998e-01  3.95650007e-02\n",
      " -4.23090011e-01 -1.57150000e-01  2.04009995e-01  1.69060007e-01\n",
      "  3.44650000e-01 -4.22619998e-01  1.95529997e-01  5.94540000e-01\n",
      " -3.05310011e-01 -1.06330000e-01 -1.90549999e-01 -5.85439980e-01\n",
      "  2.13569999e-01  3.84140015e-01  9.14990008e-02  3.83529991e-01\n",
      "  2.90749997e-01  2.45190002e-02  2.84399986e-01  6.37150034e-02\n",
      " -1.54829994e-01  4.00310010e-01  3.15429986e-01 -3.71280015e-02\n",
      "  6.33630008e-02 -2.70900011e-01  2.51599997e-01  4.71049994e-01\n",
      "  4.95559990e-01 -3.64010006e-01  1.03699997e-01  4.60759997e-02\n",
      "  1.65649995e-01 -2.90239990e-01 -6.69490024e-02 -3.08809996e-01\n",
      "  4.82630014e-01  3.09720010e-01 -1.11450002e-01 -1.03289999e-01\n",
      "  2.85850000e-02 -1.35790005e-01  5.29240012e-01 -1.40770003e-01\n",
      "  9.17629972e-02  1.31270006e-01 -2.09439993e-01  2.23270003e-02\n",
      " -7.76920021e-02  7.79339969e-02 -3.30669992e-02  1.16800003e-01\n",
      "  3.20289999e-01  3.77490014e-01 -7.56789982e-01 -1.59439996e-01\n",
      "  1.49639994e-01  4.22529995e-01  2.81360000e-03  2.13280007e-01\n",
      "  8.67760032e-02 -5.27039990e-02 -4.08589989e-01 -1.17739998e-01\n",
      "  9.06210020e-02 -2.37939999e-01 -1.83259994e-01  1.31150007e-01\n",
      " -5.59490025e-01  9.20709968e-02 -3.95039991e-02  1.33340001e-01\n",
      "  4.96320009e-01  2.87330002e-01 -1.85440004e-01  2.46179998e-02\n",
      " -4.28259999e-01  7.41479993e-02  7.65839999e-04  2.39500001e-01\n",
      "  2.26150006e-01  5.51659986e-02 -7.50960037e-02 -2.23079994e-01\n",
      "  2.37749994e-01 -4.54549998e-01  2.65639991e-01 -1.51370004e-01\n",
      " -2.41459996e-01 -2.47360006e-01  5.52139997e-01  2.68189996e-01\n",
      "  4.88310009e-01 -1.34230003e-01 -1.59180000e-01  3.76060009e-01\n",
      " -1.98339999e-01  1.66989997e-01 -1.53679997e-01  2.45609999e-01\n",
      " -9.25059989e-02 -3.02569985e-01 -2.94930011e-01 -7.49170005e-01\n",
      "  1.05669999e+00  3.79709989e-01  6.93139970e-01 -3.16720009e-02\n",
      "  2.15880007e-01 -4.07389998e-01 -1.52640000e-01  3.22959989e-01\n",
      " -1.29989997e-01 -5.01290023e-01 -4.42310005e-01  1.69040002e-02\n",
      " -1.14590004e-02  7.22930022e-03  1.10260002e-01  2.15680003e-01\n",
      " -3.23729992e-01 -3.72920007e-01 -9.24559962e-03 -2.67690003e-01\n",
      "  3.90659988e-01  3.57419997e-01 -6.06320016e-02  6.79659992e-02\n",
      "  3.38299990e-01  6.57470003e-02  1.57940000e-01  4.71550003e-02\n",
      "  2.36819997e-01 -9.13700014e-02  6.46489978e-01 -2.54909992e-01\n",
      " -6.79400027e-01 -6.97520018e-01 -1.01450004e-01 -3.62549990e-01\n",
      "  3.69670004e-01 -4.12950009e-01  8.27239975e-02 -3.50529999e-01\n",
      " -1.75640002e-01  8.50950032e-02 -5.77239990e-01  5.02520025e-01\n",
      "  5.21799982e-01  5.73269986e-02 -7.97540009e-01 -3.77700001e-01\n",
      "  7.81490028e-01  2.45969996e-01  6.06719971e-01 -2.00819999e-01\n",
      " -3.87919992e-01  4.12950009e-01 -1.61430001e-01  1.04270000e-02\n",
      "  4.31970000e-01  4.62969998e-03  2.11850002e-01 -2.66059995e-01\n",
      " -5.87400012e-02 -5.10029972e-01  2.85239995e-01  1.36270002e-02\n",
      " -2.73460001e-01  6.18479997e-02 -5.79010010e-01 -5.11359990e-01\n",
      "  3.63819987e-01  3.51440012e-01 -1.65010005e-01 -4.60409999e-01\n",
      " -6.47419989e-02 -6.83099985e-01 -4.74269986e-02  1.58610001e-01\n",
      " -4.72880006e-01  3.39679986e-01  1.20920001e-03  1.60180002e-01\n",
      " -5.80240011e-01  1.45559996e-01 -9.13169980e-01 -3.75919998e-01\n",
      " -3.29499990e-01  5.34650028e-01  1.82239994e-01 -5.22650003e-01\n",
      " -2.62089998e-01 -4.24580008e-01 -1.80340007e-01  9.95019972e-02\n",
      " -1.51140004e-01 -6.67309999e-01  2.44829997e-01 -5.66299975e-01\n",
      "  3.38429987e-01  4.05580014e-01  1.80730000e-01  6.42499983e-01]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix[word_index_eng.get(\"hello\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index_eng) + 1,\n",
    "                            embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=44,\n",
    "                            trainable=False))\n",
    "model.add(Bidirectional(LSTM(1024, dropout=0.2, recurrent_dropout=0.25,return_sequences=True),merge_mode=\"sum\"))\n",
    "model.add(LSTM(512, dropout=0.2, recurrent_dropout=0.25))\n",
    "model.add(RepeatVector(76))\n",
    "model.add(LSTM(1024, dropout=0.2, recurrent_dropout=0.25, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(34240, activation='softmax')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 44, 300)           4758600   \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 44, 1024)          10854400  \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 512)               3147776   \n",
      "_________________________________________________________________\n",
      "repeat_vector_5 (RepeatVecto (None, 76, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 76, 1024)          6295552   \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 76, 34240)         35096000  \n",
      "=================================================================\n",
      "Total params: 60,152,328\n",
      "Trainable params: 55,393,728\n",
      "Non-trainable params: 4,758,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rajar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\rajar\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 133431 samples, validate on 33358 samples\n",
      "Epoch 1/30\n",
      "   416/133431 [..............................] - ETA: 620:15:30 - loss: 4.2749 - acc: 0.8438"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('model-{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')\n",
    "\n",
    "history = model.fit(X_train_pad, y_train, batch_size=32, epochs=30, callbacks=[checkpoint], validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
